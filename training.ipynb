{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from policy_gradient import PolicyGradient\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from statistics import median, mean\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import game env\n",
    "from puzzle import GameGrid\n",
    "env = GameGrid()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_ENV = True\n",
    "EPISODES = 500\n",
    "rewards = []\n",
    "RENDER_REWARD_MIN = 50\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/arihantparsoya/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/arihantparsoya/Documents/Advanced ML/Project/CS726-Project-2048-Using-RL/policy_gradient.py:151: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "load_path = None #\"output/weights/CartPole-v0.ckpt\"\n",
    "save_path = \"output/weights/experiment2.ckpt\"\n",
    "\n",
    "PG = PolicyGradient(\n",
    "    n_x = 16,\n",
    "    n_y = 4,\n",
    "    learning_rate=LR,\n",
    "    reward_decay=0.95,\n",
    "    load_path=load_path,\n",
    "    save_path=save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(EPISODES):\n",
    "    print(\"Episode: \", episode)\n",
    "    observation = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        if RENDER_ENV: env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "# Test the policy network\n",
    "num_games = 1000\n",
    "goal_steps = 200\n",
    "rewards = []\n",
    "choices = []\n",
    "accepted_scores = []\n",
    "\n",
    "for each_game in range(num_games):\n",
    "    score = 0\n",
    "    game_memory = []\n",
    "    prev_obs = []\n",
    "    env.reset()\n",
    "    for _ in range(goal_steps):\n",
    "        #env.render()\n",
    "        if len(prev_obs)==0:\n",
    "            action = random.randrange(0,4)\n",
    "        else:\n",
    "            action = np.argmax(model.predict(prev_obs.reshape(-1,len(prev_obs),1))[0]).item()\n",
    "                \n",
    "        choices.append(action)\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        new_observation = np.array(np.array(new_observation).flatten().tolist())\n",
    "        \n",
    "        prev_obs = new_observation\n",
    "        game_memory.append([new_observation, action])\n",
    "        \n",
    "        score+=reward\n",
    "        if done: break\n",
    "        \n",
    "    if each_game%100 == 0: print(each_game)\n",
    "    rewards.append(score)\n",
    "    accepted_scores.append(env.highest_score())\n",
    "\n",
    "print('Average Score:',sum(rewards)/len(rewards))\n",
    "print('choice 0:{} choice 1:{} choice 2:{} choice 3:{}'.format(choices.count(0)/len(choices),choices.count(1)/len(choices), choices.count(2)/len(choices), choices.count(3)/len(choices)))\n",
    "print(Counter(accepted_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
