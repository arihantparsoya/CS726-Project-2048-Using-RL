{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import random\n",
    "import numpy as np\n",
    "from statistics import median, mean\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output # only for jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import game env\n",
    "from puzzle import GameGrid\n",
    "env = GameGrid()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "[2, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# display current state\n",
    "env.display_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Random Data\n",
    "To check if its working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_random_games_first():\n",
    "    # Each of these is its own game.\n",
    "    for episode in range(5):\n",
    "        env.reset()\n",
    "        # this is each frame, up to 200...but we wont make it that far.\n",
    "        for t in range(200):\n",
    "            # This will display the environment\n",
    "            # Only display if you really want to see it.\n",
    "            # Takes much longer to display it.\n",
    "            clear_output()\n",
    "            env.display_state()\n",
    "            \n",
    "            # This will just create a sample action in any environment.\n",
    "            # In this environment, the action can be 0 or 1, which is left or right\n",
    "            action = env.action_space()\n",
    "            \n",
    "            # this executes the environment with an action, \n",
    "            # and returns the observation of the environment, \n",
    "            # the reward, if the env is over, and other info.\n",
    "            \n",
    "            observation, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2, 2, 8, 4]\n",
      "[16, 32, 2, 32]\n",
      "[4, 2, 16, 4]\n",
      "[2, 32, 4, 8]\n"
     ]
    }
   ],
   "source": [
    "some_random_games_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accepted score: 11241.985\n",
      "Median score for accepted rewards: 9706.0\n",
      "Counter({128: 5026, 64: 3477, 256: 954, 32: 524, 16: 18, 512: 1})\n"
     ]
    }
   ],
   "source": [
    "score_requirement = 16 # Save (state, action) pair only if score is higher than score_requirement\n",
    "initial_games = 10000 # number of games played\n",
    "goal_steps = 10000 # number of steps in each game\n",
    "\n",
    "# [OBS, MOVES]\n",
    "training_data = []\n",
    "# all rewards:\n",
    "rewards = []\n",
    "# just the rewards that met our threshold:\n",
    "accepted_rewards = []\n",
    "# list of top scores\n",
    "accepted_scores = []\n",
    "# iterate through however many games we want:\n",
    "for _ in range(initial_games):\n",
    "    score = 0\n",
    "    # moves specifically from this environment:\n",
    "    game_memory = []\n",
    "    # previous observation that we saw\n",
    "    prev_observation = []\n",
    "    # for each frame in 200\n",
    "    for _ in range(goal_steps):\n",
    "        # choose random action (0 or 1)\n",
    "        action = env.action_space()\n",
    "        # do it!\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # notice that the observation is returned FROM the action\n",
    "        # so we'll store the previous observation here, pairing\n",
    "        # the prev observation to the action we'll take.\n",
    "        if len(prev_observation) > 0 :\n",
    "            game_memory.append([prev_observation, action])\n",
    "        prev_observation = observation\n",
    "        score+=reward\n",
    "        if done: break\n",
    "\n",
    "    # IF our score is higher than our threshold, we'd like to save\n",
    "    # every move we made\n",
    "    # NOTE the reinforcement methodology here. \n",
    "    # all we're doing is reinforcing the score, we're not trying \n",
    "    # to influence the machine in any way as to HOW that score is \n",
    "    # reached.\n",
    "    if score >= score_requirement:\n",
    "        accepted_rewards.append(score)\n",
    "        accepted_scores.append(env.highest_score())\n",
    "        for data in game_memory:\n",
    "            # Create one hot vector for actions\n",
    "            # [\"'w'\", \"'s'\", \"'d'\", \"'a'\"] === [UP, DOWN, RIGHT, LEFT]\n",
    "            if data[1] == \"'w'\":\n",
    "                output = [1,0,0,0]\n",
    "            elif data[1] == \"'s'\":\n",
    "                output = [0,1,0,0]\n",
    "            elif data[1] == \"'d'\":\n",
    "                output = [0,0,1,0]\n",
    "            elif data[1] == \"'a'\":\n",
    "                output = [0,0,0,1]\n",
    "\n",
    "            # saving our training data\n",
    "            training_data.append([np.array(data[0]).flatten().tolist(), output])\n",
    "\n",
    "    # reset env to play again\n",
    "    env.reset()\n",
    "    # save overall scores\n",
    "    rewards.append(score)\n",
    "\n",
    "# just in case you wanted to reference later\n",
    "training_data_save = np.array(training_data)\n",
    "np.save('data/saved.npy',training_data_save)\n",
    "\n",
    "# some stats here, to further illustrate the neural network magic!\n",
    "print(\"Training Points\", len(training_data))\n",
    "print('Average accepted score:', mean(accepted_rewards))\n",
    "print('Median score for accepted rewards:',median(accepted_rewards))\n",
    "print(Counter(accepted_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
